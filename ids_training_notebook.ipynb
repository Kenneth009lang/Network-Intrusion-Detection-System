from IPython import get_ipython
from IPython.display import display

# code adapted from Pedregosa et al. (2011) – scikit-learn core utilities
# core libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os, time

# Pedregosa et al. (2011) – scikit-learn model selection and preprocessing
from sklearn.model_selection import (
    train_test_split,
    StratifiedKFold
)
from sklearn.preprocessing import (
    LabelEncoder,
    MinMaxScaler
)
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score
)
from sklearn.ensemble import RandomForestClassifier
# end of adapted code

# code adapted from Chawla et al. (2002) – SMOTE for class imbalance handling
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
# end of adapted code

# Chollet (2015) – Keras for deep learning model
import tensorflow as tf
from tensorflow.keras.models import Sequential
# code adapted from LeCun et al. (1998); Hochreiter & Schmidhuber (1997); Schuster & Paliwal (1997)
# – CNN, LSTM, Bidirectional RNN layers
from tensorflow.keras.layers import (
    Dense,
    LSTM,
    Bidirectional,
    Conv1D,
    MaxPooling1D,
    Dropout
)
# end of adapted code

# code adapted from Srivastava et al. (2014) – Dropout regularization
from tensorflow.keras.callbacks import EarlyStopping
# end of adapted code

# code adapted from Lundberg & Lee (2017); Ribeiro et al. (2016) – SHAP & LIME explainability tools
import shap
import lime
import lime.lime_tabular
# end of adapted code

# reproducibility
np.random.seed(42)
tf.random.set_seed(42)
print("All libraries ready to go")

# code adapted from Sharafaldin et al. (2018) – loading CICIDS2017 intrusion detection dataset
from google.colab import drive
drive.mount('/content/drive')

data_path = '/content/drive/My Drive/ml cve/MachineLearningCVE'
print("Reading from:", data_path)

# multiple CSVs containing network traffic sessions
cicids_files = [
    'Monday-WorkingHours.pcap_ISCX.csv',
    'Tuesday-WorkingHours.pcap_ISCX.csv',
    'Wednesday-WorkingHours.pcap_ISCX.csv',
    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',
    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',
    'Friday-WorkingHours-Morning.pcap_ISCX.csv',
    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',
    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'
]

# code adapted from McKinney (2010) – pandas for data handling and cleaning
df_list = []
for f in cicids_files:
    fpath = os.path.join(data_path, f)
    print("Loading", f)
    try:
        if 'Thursday-WorkingHours-Morning-WebAttacks' in f:
            temp = pd.read_csv(fpath, encoding="latin1")
            temp['Label'] = temp['Label'].replace({
                'Web Attack \x96 Brute Force': 'Web Attack-Brute Force',
                'Web Attack \x96 XSS': 'Web Attack-XSS',
                'Web Attack \x96 Sql Injection': 'Web Attack-Sql Injection'
            })
        else:
            temp = pd.read_csv(fpath)
        df_list.append(temp)
        print(f, "OK ->", len(temp), "rows")
    except Exception as e:
        print("Problem with", f, ":", e)
if not df_list:
    raise ValueError("No files loaded, check path/names")

# combine all daily traffic data into one DataFrame
df = pd.concat(df_list, ignore_index=True)
print("Combined shape:", df.shape)

# clean column names
df.columns = df.columns.str.strip()
if ' Label' in df.columns:
    df.rename(columns={' Label': 'Label'}, inplace=True)

# drop unnecessary identifier columns – Pedregosa et al. (2011)
drop_cols = ['Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Timestamp']
df = df.drop(columns=drop_cols, errors="ignore")
print("After dropping ID cols:", df.shape)

# McKinney (2010) – clean infinities and NaNs
df.replace([np.inf, -np.inf], np.nan, inplace=True)
before = df.shape[0]
df.dropna(inplace=True)
print("Dropped", before - df.shape[0], "rows with NaNs")

# remove duplicate rows
before = df.shape[0]
df.drop_duplicates(inplace=True)
print("Removed", before - df.shape[0], "duplicates")

# separate features and labels
X = df.drop('Label', axis=1)
y = df['Label']

# force numeric conversion (McKinney, 2010)
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')
X.dropna(inplace=True)
y = y[X.index]
print("Data after cleanup:", X.shape)
# end of adapted code for data handling and cleaning

# code adapted from Pedregosa et al. (2011) – encode categorical labels
le = LabelEncoder()
y_enc = le.fit_transform(y)
print("Classes:", le.classes_)

# Pedregosa et al. (2011) – train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42
)
print("Train size:", X_train.shape, " Test size:", X_test.shape)
# end of adapted code for train/test split

# code adapted from Chawla et al. (2002) – apply SMOTE for class balancing
sm = SMOTE(random_state=42)
X_train, y_train = sm.fit_resample(X_train, y_train)
print("After SMOTE:", X_train.shape)
# end of adapted code for SMOTE

# code adapted from Pedregosa et al. (2011) – scale features to [0, 1]
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# end of adapted code for feature scaling

# code adapted from LeCun et al. (1998); Hochreiter & Schmidhuber (1997) – reshape for CNN/LSTM input
X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)
X_test = X_test.reshape(len(X_test), X_test.shape[1], 1)
print("After reshape:", X_train.shape, X_test.shape)
# end of adapted code for input reshaping

#  build CNN + BiLSTM model 
# code adapted from LeCun et al. (1998) – CNN
# code adapted from Hochreiter & Schmidhuber (1997) – LSTM
# code adapted from Schuster & Paliwal (1997) – Bidirectional RNN
# code adapted from Srivastava et al. (2014) – Dropout
# code adapted from Kingma & Ba (2014) – Adam optimizer
model = Sequential()
model.add(Conv1D(64, 3, activation="relu", input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Bidirectional(LSTM(64)))
model.add(Dropout(0.3))
model.add(Dense(64, activation="relu"))
model.add(Dense(len(np.unique(y_enc)), activation="softmax"))
model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)
model.summary()

# train model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=10, batch_size=64
)
# end of adapted code for deep learning model structure and training

# code adapted from Pedregosa et al. (2011) – evaluate performance
y_pred = np.argmax(model.predict(X_test), axis=1)
print("Classification report:\n", classification_report(y_test, y_pred, target_names=le.classes_))

# plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix")
plt.show()
# end of adapted code for model evaluation

# code adapted from Lundberg & Lee (2017) – SHAP explainability
expl = shap.DeepExplainer(model, X_train[:200])
shap_vals = expl.shap_values(X_test[:50])
shap.summary_plot(shap_vals, X_test[:50], feature_names=X.columns)
# end of adapted code for SHAP explainability 
# Artifact Saving Code
# Imports
# Adapted Code from: Tensorflow, 2025
import os
import time
import json
import joblib

from sklearn.metrics import (
    classification_report
)
# End of adapted code
# Directory Setup
timestamp = time.strftime("%Y%m%d")
save_dir = f"/content/drive/My Drive/ml_cve_artifacts/{timestamp}_cnn_bilstm"
os.makedirs(save_dir, exist_ok=True)
print(f"Saving required artifacts to: {save_dir}")
# Adapted Code  from: Tensorflow – Keras model saving
# 1. To save the trained Keras Model 
model_path = os.path.join(save_dir, "cnn_bilstm_model.keras")
model.save(model_path)
print(f"Model saved to: {model_path}")
# 2. Save the MinMaxScaler (Feature Scaler) 
scaler_path = os.path.join(save_dir, "minmax_scaler.joblib")
joblib.dump(scaler, scaler_path)
print(f"Scaler saved to: {scaler_path}")
# 3. Save the LabelEncoder (Target Encoder) 
encoder_path = os.path.join(save_dir, "label_encoder.joblib")
joblib.dump(le, encoder_path)
print(f"Encoder saved to: {encoder_path}")
# 4. Save the Classification Report 
report_dict = classification_report(
    y_test, y_pred, target_names=le.classes_, output_dict=True
)
report_path = os.path.join(save_dir, "classification_report.json")
with open(report_path, "w") as f:
    json.dump(report_dict, f, indent=4)
print(f"Report was saved to: {report_path}")
# 5. Save the Feature Names (Optional, but helpful) 
features_path = os.path.join(save_dir, "feature_names.json")
with open(features_path, "w") as f:
    json.dump(X.columns.tolist(), f, indent=4)
print(f"Feature was saved to: {features_path}")
# End of adapted code
print("Artifacts saved successfully.")

